{
  /*Class name of the model to use for training
    Search in core.py */
  model: "NewtonianVAE",

  NewtonianVAE: {
    dim_x: 2,

    /*Use KL(q(x|I)‖N (0, 1))
      undefined == false
      Paper: Secondly, we added an additional regularization term to the latent space, KL(q(x|I)‖N (0, 1)) */
    // regularization: false,

    camera_name: "top",

    // You can try it!
    // decoder_free: true, // undefined == false

    velocity: {
      /*Fix values for A, B, C
        Paper: Firstly, the transition matrices were set to A = 0, B = 0, C = 1. */
      fix_abc: [0, 0, 1],
      // fix_abc: null,  // train ABC
    },

    transition: {
      std: 0.001,
    },

    encoder: {
      // image -> model (enc) -> nn.Linear -> μ, σ

      model: "VanillaEncoder",
      model_kwargs: {
        dim_output: 1024,
      },

      // You can try these!

      // model: "ResNet",
      // model_kwargs: {
      //   dim_output: 128,
      // },

      // model: "MobileViTWrap",
      // model_kwargs: {
      //   dim_output: 128,
      //   img_size: [64, 64]
      // }
    },

    decoder: {
      // x (latent) -> model (dec) -> generated image

      model: "VanillaDecoder",
    },
  },

  train: {
    /*
      Paper:

      To train the models, we generate 1000 random se-
      quences with 100 time-steps for the point mass and
      reacher-2D systems, and 30 time-steps for the fetch-3D
      system.

      All
      models were trained using Adam [28] with a learning
      rate of 3 · 10−4 and batch size 1 (a single sequence per
      batch) for 300 epochs.
    */

    /*Support: https://facelessuser.github.io/wcmatch/glob/
      Multiple paths can also be written using arrays
      You can use symbolic link for second disk */
    path: "data/reacher2d/episodes/{0..999}.*",

    batch_size: 20, // 1 is slow
    epochs: 300,
    learning_rate: 3e-4,

    /*Max time length
      "clip_min" : Truncate from batch data according to minimum time length */
    // max_time_length: 100,
    // max_time_length: "clip_min",
    max_time_length: 70,

    device: "cuda",
    dtype: "float32",
    check_value: false, // if false, a little faster

    // Per epoch to save the weights
    save_per_epoch: 50,

    /*Random seed  (Optional)
      int or null
      undefined == null
      If null, the seed is determined automatically. */
    // seed: null,

    /*https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html
      Number type or null
      undefined == null */
    // grad_clip_norm: null,

    // You can try AMP!
    // gradscaler_args: {}, // torch.cuda.amp.GradScaler, undefined == null
    // use_autocast: true, // torch.cuda.amp.autocast, undefined == false

    // Whether to load all data first
    load_all: true, // undefined == false
  },

  valid: {
    path: "data/reacher2d/episodes/{1000..1199}.*",
  },

  test: {
    path: "data/reacher2d/episodes/{1200..1399}.*",
    device: "cuda",
    dtype: "float32", // Probably float16 is OK
  },

  sim_env: "ControlSuite",

  ControlSuite: {
    env: "reacher2d-hard",
    seed: 1,
    max_episode_length: 100,
    imgsize: [64, 64], // (H, W), for saving
    // ***
    // action_type: "default",
    // position_wrap: "endeffector",
    // xml_file: "*.xml",
  },

  path: {
    saves_dir: "save/reacher2d/saves", // For model weight & this file (hyperparameter)
    results_dir: "save/reacher2d/results", // For visualization results using model
  },

  others: {
    // Ignored on create_data.py
    keypaths: [
      "action", //
      "delta",
      "position",
      ["camera", "top"],
    ],

    /*PrePostPrpcess
      Ignored on create_data.py for data saving */
    HandyForImage: {
      size: [64, 64], // for training
      bit_depth: 5,
    },
  },
}
